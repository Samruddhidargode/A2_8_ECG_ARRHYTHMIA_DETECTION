"""
Comprehensive ML Training Module for AIML Project
Shows progression: Basic ‚Üí Intermediate ‚Üí Advanced models
"""

import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (classification_report, confusion_matrix, 
                             accuracy_score, f1_score, precision_recall_fscore_support)
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ===== BASIC MODELS (AIML Fundamentals) =====
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.tree import DecisionTreeClassifier

# ===== INTERMEDIATE MODELS (Ensemble Methods) =====
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import (RandomForestClassifier, 
                              AdaBoostClassifier,
                              BaggingClassifier)

# ===== ADVANCED MODELS (Gradient Boosting) =====
from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è  XGBoost not available, will skip")

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è  LightGBM not available, will skip")

import joblib
import os
import time


class ComprehensiveArrhythmiaClassifier:
    """
    Complete ML classifier showing all model types
    Organized by complexity level for educational purposes
    """
    
    def __init__(self):
        self.models = {}
        self.training_times = {}
        self.model_categories = {
            'basic': [],
            'intermediate': [],
            'advanced': []
        }
        
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.best_model_name = None
        self.best_model = None
        
    def balance_dataset(self, X, y, method='smote'):
        """Balance dataset using SMOTE"""
        print("\n" + "="*70)
        print(f"‚öñÔ∏è  BALANCING DATASET USING {method.upper()}")
        print("="*70)
        
        print("\nüìä Original class distribution:")
        original_dist = Counter(y)
        total_original = sum(original_dist.values())
        for cls, count in sorted(original_dist.items()):
            print(f"   {cls}: {count:5d} ({count/total_original*100:5.1f}%)")
        
        # Apply SMOTE
        try:
            balancer = SMOTE(random_state=42, k_neighbors=5)
            X_balanced, y_balanced = balancer.fit_resample(X, y)
            
            print("\nüìä Balanced class distribution:")
            balanced_dist = Counter(y_balanced)
            total_balanced = sum(balanced_dist.values())
            for cls, count in sorted(balanced_dist.items()):
                change = count - original_dist.get(cls, 0)
                print(f"   {cls}: {count:5d} ({count/total_balanced*100:5.1f}%) [+{change}]")
            
            print(f"\n‚úÖ Dataset balanced: {len(y):,} ‚Üí {len(y_balanced):,} samples")
            return X_balanced, y_balanced
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Balancing failed: {e}")
            print("   Using original dataset")
            return X, y
    
    def prepare_data(self, X, y, test_size=0.2, balance_train=True):
        """Prepare data with optional balancing"""
        print("\n" + "="*70)
        print("üìä DATA PREPARATION")
        print("="*70)
        
        # Check for NaN values
        print("\nüîç Checking data quality...")
        nan_count = np.isnan(X).sum()
        if nan_count > 0:
            print(f"   ‚ö†Ô∏è  Found {nan_count} NaN values")
            print("   üîß Cleaning data...")
            
            # Strategy 1: Replace NaN with column mean
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='mean')
            X = imputer.fit_transform(X)
            
            print(f"   ‚úì Replaced NaN with column means")
            
            # Verify no more NaN
            remaining_nan = np.isnan(X).sum()
            if remaining_nan > 0:
                print(f"   ‚ö†Ô∏è  Still {remaining_nan} NaN, removing affected rows...")
                valid_rows = ~np.isnan(X).any(axis=1)
                X = X[valid_rows]
                y = y[valid_rows]
                print(f"   ‚úì Removed rows, remaining: {len(y)} samples")
        else:
            print("   ‚úì No NaN values found")
        
        # Check for infinite values
        inf_count = np.isinf(X).sum()
        if inf_count > 0:
            print(f"   ‚ö†Ô∏è  Found {inf_count} infinite values")
            print("   üîß Replacing with max finite values...")
            X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)
            print("   ‚úì Cleaned infinite values")
        
        print(f"\n‚úÖ Clean data: {len(y)} samples, {X.shape[1]} features")
        
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Split data (stratified)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded,
            test_size=test_size,
            random_state=42,
            stratify=y_encoded
        )
        
        print(f"\nInitial split:")
        print(f"  Training: {len(X_train):,} samples")
        print(f"  Testing:  {len(X_test):,} samples")
        
        # Balance training set if requested
        if balance_train:
            y_train_labels = self.label_encoder.inverse_transform(y_train)
            X_train_balanced, y_train_balanced = self.balance_dataset(
                X_train, y_train_labels, method='smote'
            )
            y_train = self.label_encoder.transform(y_train_balanced)
            X_train = X_train_balanced
        
        # Scale features
        print("\nüîÑ Scaling features...")
        X_train = self.scaler.fit_transform(X_train)
        X_test = self.scaler.transform(X_test)
        
        print(f"\n‚úÖ Data ready:")
        print(f"   Training samples: {len(X_train):,}")
        print(f"   Testing samples:  {len(X_test):,}")
        print(f"   Features: {X_train.shape[1]}")
        print(f"   Classes: {len(np.unique(y_encoded))}")
        
        return X_train, X_test, y_train, y_test
    
    # ========== BASIC MODELS ==========
    
    def train_logistic_regression(self, X_train, y_train):
        """1. Logistic Regression - Linear classification"""
        print("\nüìä [BASIC] Training Logistic Regression...")
        print("   Theory: Linear decision boundary, probabilistic output")
        
        start_time = time.time()
        model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            solver='lbfgs',
            multi_class='multinomial',
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        self.training_times['logistic_regression'] = time.time() - start_time
        
        self.models['logistic_regression'] = model
        self.model_categories['basic'].append('logistic_regression')
        print(f"   ‚úì Trained in {self.training_times['logistic_regression']:.2f}s")
        return model
    
    def train_naive_bayes(self, X_train, y_train):
        """2. Naive Bayes - Probabilistic classifier"""
        print("\nüìä [BASIC] Training Naive Bayes...")
        print("   Theory: Assumes feature independence, uses Bayes theorem")
        
        start_time = time.time()
        model = GaussianNB()
        model.fit(X_train, y_train)
        self.training_times['naive_bayes'] = time.time() - start_time
        
        self.models['naive_bayes'] = model
        self.model_categories['basic'].append('naive_bayes')
        print(f"   ‚úì Trained in {self.training_times['naive_bayes']:.2f}s")
        return model
    
    def train_decision_tree(self, X_train, y_train):
        """3. Decision Tree - Rule-based classifier"""
        print("\nüå≥ [BASIC] Training Decision Tree...")
        print("   Theory: Creates binary tree of if-else rules")
        
        start_time = time.time()
        model = DecisionTreeClassifier(
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        self.training_times['decision_tree'] = time.time() - start_time
        
        self.models['decision_tree'] = model
        self.model_categories['basic'].append('decision_tree')
        print(f"   ‚úì Trained in {self.training_times['decision_tree']:.2f}s")
        return model
    
    def train_perceptron(self, X_train, y_train):
        """4. Perceptron - Single layer neural network"""
        print("\nüß† [BASIC] Training Perceptron...")
        print("   Theory: Linear model, basis of neural networks")
        
        start_time = time.time()
        model = Perceptron(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        self.training_times['perceptron'] = time.time() - start_time
        
        self.models['perceptron'] = model
        self.model_categories['basic'].append('perceptron')
        print(f"   ‚úì Trained in {self.training_times['perceptron']:.2f}s")
        return model
    
    # ========== INTERMEDIATE MODELS ==========
    
    def train_knn(self, X_train, y_train):
        """5. K-Nearest Neighbors - Instance-based learning"""
        print("\nüë• [INTERMEDIATE] Training K-Nearest Neighbors...")
        print("   Theory: Classifies based on k closest training examples")
        
        start_time = time.time()
        model = KNeighborsClassifier(
            n_neighbors=5,
            weights='distance',
            metric='euclidean',
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        self.training_times['knn'] = time.time() - start_time
        
        self.models['knn'] = model
        self.model_categories['intermediate'].append('knn')
        print(f"   ‚úì Trained in {self.training_times['knn']:.2f}s")
        return model
    
    def train_svm(self, X_train, y_train):
        """6. Support Vector Machine - Margin maximization"""
        print("\nüéØ [INTERMEDIATE] Training SVM...")
        print("   Theory: Finds optimal hyperplane with maximum margin")
        
        start_time = time.time()
        model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        self.training_times['svm'] = time.time() - start_time
        
        self.models['svm'] = model
        self.model_categories['intermediate'].append('svm')
        print(f"   ‚úì Trained in {self.training_times['svm']:.2f}s")
        return model
    
    def train_random_forest(self, X_train, y_train):
        """7. Random Forest - Ensemble of decision trees"""
        print("\nüå≤ [INTERMEDIATE] Training Random Forest...")
        print("   Theory: Bagging - combines multiple decision trees")
        
        start_time = time.time()
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        self.training_times['random_forest'] = time.time() - start_time
        
        self.models['random_forest'] = model
        self.model_categories['intermediate'].append('random_forest')
        print(f"   ‚úì Trained in {self.training_times['random_forest']:.2f}s")
        return model
    
    def train_adaboost(self, X_train, y_train):
        """8. AdaBoost - Adaptive boosting"""
        print("\nüìà [INTERMEDIATE] Training AdaBoost...")
        print("   Theory: Boosting - focuses on misclassified samples")
        
        start_time = time.time()
        model = AdaBoostClassifier(
            n_estimators=100,
            learning_rate=1.0,
            random_state=42
        )
        model.fit(X_train, y_train)
        self.training_times['adaboost'] = time.time() - start_time
        
        self.models['adaboost'] = model
        self.model_categories['intermediate'].append('adaboost')
        print(f"   ‚úì Trained in {self.training_times['adaboost']:.2f}s")
        return model
    
    # ========== ADVANCED MODELS ==========
    
    def train_gradient_boosting(self, X_train, y_train):
        """9. Gradient Boosting - Sequential ensemble"""
        print("\nüöÄ [ADVANCED] Training Gradient Boosting...")
        print("   Theory: Sequential boosting with gradient descent")
        
        start_time = time.time()
        model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            random_state=42
        )
        model.fit(X_train, y_train)
        self.training_times['gradient_boosting'] = time.time() - start_time
        
        self.models['gradient_boosting'] = model
        self.model_categories['advanced'].append('gradient_boosting')
        print(f"   ‚úì Trained in {self.training_times['gradient_boosting']:.2f}s")
        return model
    
    def train_xgboost(self, X_train, y_train):
        """10. XGBoost - Extreme Gradient Boosting"""
        if not XGBOOST_AVAILABLE:
            print("\n‚ö†Ô∏è  [ADVANCED] XGBoost not available, skipping")
            return None
            
        print("\n‚ö° [ADVANCED] Training XGBoost...")
        print("   Theory: Optimized gradient boosting with regularization")
        
        start_time = time.time()
        model = XGBClassifier(
            n_estimators=100,
            max_depth=7,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='mlogloss',
            use_label_encoder=False
        )
        model.fit(X_train, y_train)
        self.training_times['xgboost'] = time.time() - start_time
        
        self.models['xgboost'] = model
        self.model_categories['advanced'].append('xgboost')
        print(f"   ‚úì Trained in {self.training_times['xgboost']:.2f}s")
        return model
    
    def train_lightgbm(self, X_train, y_train):
        """11. LightGBM - Light Gradient Boosting Machine"""
        if not LIGHTGBM_AVAILABLE:
            print("\n‚ö†Ô∏è  [ADVANCED] LightGBM not available, skipping")
            return None
            
        print("\nüí° [ADVANCED] Training LightGBM...")
        print("   Theory: Fast gradient boosting using histogram-based learning")
        
        start_time = time.time()
        model = LGBMClassifier(
            n_estimators=100,
            max_depth=7,
            learning_rate=0.1,
            num_leaves=31,
            random_state=42,
            verbose=-1
        )
        model.fit(X_train, y_train)
        self.training_times['lightgbm'] = time.time() - start_time
        
        self.models['lightgbm'] = model
        self.model_categories['advanced'].append('lightgbm')
        print(f"   ‚úì Trained in {self.training_times['lightgbm']:.2f}s")
        return model
    
    def train_all_models(self, X_train, y_train, skip_slow=False):
        """Train all models organized by category"""
        print("\n" + "="*70)
        print("ü§ñ COMPREHENSIVE MODEL TRAINING")
        print("   Demonstrating: Basic ‚Üí Intermediate ‚Üí Advanced")
        print("="*70)
        
        # BASIC MODELS
        print("\n" + "="*70)
        print("üìö PHASE 1: BASIC MODELS (Fundamentals)")
        print("="*70)
        self.train_logistic_regression(X_train, y_train)
        self.train_naive_bayes(X_train, y_train)
        self.train_decision_tree(X_train, y_train)
        self.train_perceptron(X_train, y_train)
        
        # INTERMEDIATE MODELS
        print("\n" + "="*70)
        print("üéì PHASE 2: INTERMEDIATE MODELS (Ensemble Methods)")
        print("="*70)
        self.train_knn(X_train, y_train)
        if not skip_slow or len(X_train) < 10000:
            self.train_svm(X_train, y_train)
        else:
            print("\n‚ö†Ô∏è  Skipping SVM (slow on large datasets)")
        self.train_random_forest(X_train, y_train)
        self.train_adaboost(X_train, y_train)
        
        # ADVANCED MODELS
        print("\n" + "="*70)
        print("üöÄ PHASE 3: ADVANCED MODELS (Gradient Boosting)")
        print("="*70)
        self.train_gradient_boosting(X_train, y_train)
        self.train_xgboost(X_train, y_train)
        self.train_lightgbm(X_train, y_train)
        
        print("\n" + "="*70)
        print(f"‚úÖ TRAINING COMPLETE: {len(self.models)} models trained")
        print(f"   Basic: {len(self.model_categories['basic'])} models")
        print(f"   Intermediate: {len(self.model_categories['intermediate'])} models")
        print(f"   Advanced: {len(self.model_categories['advanced'])} models")
        print("="*70)
    
    def evaluate_model(self, model_name, model, X_test, y_test):
        """Comprehensive model evaluation"""
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='weighted', zero_division=0
        )
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'training_time': self.training_times.get(model_name, 0),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'predictions': y_pred,
            'probabilities': y_prob
        }
        
        return results
    
    def evaluate_all_models(self, X_test, y_test):
        """Evaluate all models with comparative analysis"""
        print("\n" + "="*70)
        print("üìä COMPREHENSIVE MODEL EVALUATION")
        print("="*70)
        
        results = {}
        best_f1 = 0
        
        # Evaluate by category
        for category in ['basic', 'intermediate', 'advanced']:
            if len(self.model_categories[category]) > 0:
                print(f"\n{category.upper()} MODELS:")
                print("-" * 70)
                
                for name in self.model_categories[category]:
                    if name in self.models:
                        model = self.models[name]
                        results[name] = self.evaluate_model(name, model, X_test, y_test)
                        
                        acc = results[name]['accuracy']
                        f1 = results[name]['f1_score']
                        time_taken = results[name]['training_time']
                        
                        print(f"\n{name.replace('_', ' ').title()}:")
                        print(f"  Accuracy:  {acc:.4f} ({acc*100:.2f}%)")
                        print(f"  Precision: {results[name]['precision']:.4f}")
                        print(f"  Recall:    {results[name]['recall']:.4f}")
                        print(f"  F1-Score:  {f1:.4f}")
                        print(f"  Train Time: {time_taken:.2f}s")
                        
                        # Track best model by F1 score
                        if f1 > best_f1:
                            best_f1 = f1
                            self.best_model_name = name
                            self.best_model = model
        
        # Summary
        print("\n" + "="*70)
        print("üèÜ BEST MODEL OVERALL")
        print("="*70)
        print(f"Model: {self.best_model_name.upper()}")
        print(f"Accuracy:  {results[self.best_model_name]['accuracy']:.4f}")
        print(f"F1-Score:  {results[self.best_model_name]['f1_score']:.4f}")
        
        # Determine category
        for cat, models in self.model_categories.items():
            if self.best_model_name in models:
                print(f"Category:  {cat.upper()}")
                break
        
        print("="*70)
        
        return results
    
    def create_comparison_dataframe(self, results):
        """Create DataFrame for easy comparison"""
        comparison_data = []
        
        for name, metrics in results.items():
            # Determine category
            category = 'Unknown'
            for cat, models in self.model_categories.items():
                if name in models:
                    category = cat.capitalize()
                    break
            
            comparison_data.append({
                'Model': name.replace('_', ' ').title(),
                'Category': category,
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1_score'],
                'Training Time (s)': metrics['training_time']
            })
        
        df = pd.DataFrame(comparison_data)
        df = df.sort_values('F1-Score', ascending=False)
        
        return df
    
    def save_models(self, directory='comprehensive_models'):
        """Save all trained models"""
        os.makedirs(directory, exist_ok=True)
        
        # Save models
        for name, model in self.models.items():
            filepath = os.path.join(directory, f'{name}.pkl')
            joblib.dump(model, filepath)
        
        # Save scaler and encoder
        joblib.dump(self.scaler, os.path.join(directory, 'scaler.pkl'))
        joblib.dump(self.label_encoder, os.path.join(directory, 'label_encoder.pkl'))
        
        # Save metadata
        metadata = {
            'best_model_name': self.best_model_name,
            'classes': self.label_encoder.classes_.tolist(),
            'model_categories': self.model_categories,
            'training_times': self.training_times
        }
        joblib.dump(metadata, os.path.join(directory, 'metadata.pkl'))
        
        print(f"\nüíæ All models saved to {directory}/")
        print(f"   Total models: {len(self.models)}")
    
    def predict(self, X, use_best=True, model_name=None):
        """Make predictions"""
        if use_best and self.best_model:
            model = self.best_model
        elif model_name and model_name in self.models:
            model = self.models[model_name]
        else:
            raise ValueError("No model available")
        
        X_scaled = self.scaler.transform(X)
        y_pred = model.predict(X_scaled)
        y_prob = model.predict_proba(X_scaled) if hasattr(model, 'predict_proba') else None
        
        y_pred_labels = self.label_encoder.inverse_transform(y_pred)
        
        return y_pred_labels, y_prob


# Example usage
if __name__ == "__main__":
    print("ü´Ä Comprehensive ML Training Module\n")
    print("="*70)
    print("‚ö†Ô∏è  WARNING: This demo uses SYNTHETIC data for quick testing")
    print("   For REAL results, run: python test_real_data.py")
    print("="*70)
    
    # Generate synthetic data (PERFECTLY SEPARABLE - not realistic!)
    np.random.seed(42)
    n_per_class = 300
    
    X = np.vstack([
        np.random.randn(n_per_class, 40) * 0.5,
        np.random.randn(n_per_class, 40) * 0.6 + 1.0,
        np.random.randn(n_per_class, 40) * 0.8 + 2.0
    ])
    y = np.array(['N'] * n_per_class + ['S'] * n_per_class + ['V'] * n_per_class)
    
    print("\nüìä Synthetic Data Characteristics:")
    print("   ‚Ä¢ Perfectly separable classes (mean-shifted Gaussians)")
    print("   ‚Ä¢ 300 samples per class")
    print("   ‚Ä¢ 40 features")
    print("   ‚Ä¢ Expected result: ~100% accuracy (TOO EASY!)")
    print("\n   ‚ö†Ô∏è  This is NOT representative of real ECG data!\n")
    
    # Initialize classifier
    classifier = ComprehensiveArrhythmiaClassifier()
    
    # Prepare data
    X_train, X_test, y_train, y_test = classifier.prepare_data(X, y, balance_train=True)
    
    # Train ALL models
    classifier.train_all_models(X_train, y_train)
    
    # Evaluate
    results = classifier.evaluate_all_models(X_test, y_test)
    
    # Create comparison table
    comparison_df = classifier.create_comparison_dataframe(results)
    print("\nüìä MODEL COMPARISON TABLE:")
    print(comparison_df.to_string(index=False))
    
    print("\n" + "="*70)
    print("‚ö†Ô∏è  IMPORTANT NOTE:")
    print("="*70)
    print("These results (100% accuracy) are from SYNTHETIC data.")
    print("Real ECG data will show:")
    print("   ‚Ä¢ Naive Bayes: 85-90% (feature independence assumption violated)")
    print("   ‚Ä¢ Decision Tree: 90-94% (overfitting on simple splits)")
    print("   ‚Ä¢ Random Forest: 96-98% (ensemble reduces overfitting)")
    print("   ‚Ä¢ XGBoost: 97-99% (best for complex medical data)")
    print("\nFor realistic results with REAL data, run:")
    print("   python test_real_data.py")
    print("="*70)
    
    print("\n‚úÖ Demo complete!")